{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Viewing Data\n\nThe top rows of a DataFrame can be displayed using `DataFrame.show()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show(1)","execution_count":5,"outputs":[{"output_type":"stream","text":"+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\nonly showing top 1 row\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]","text/html":"<table border='1'>\n<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n</table>\n"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.show(1, vertical=True)","execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"-RECORD 0------------------\n a   | 1                   \n b   | 2.0                 \n c   | string1             \n d   | 2000-01-01          \n e   | 2000-01-01 12:00:00 \nonly showing top 1 row\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns","execution_count":10,"outputs":[{"data":{"text/plain":"['a', 'b', 'c', 'd', 'e']"},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.printSchema()","execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"root\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.select(\"a\", \"b\", \"c\").describe().show()","execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"+-------+---+---+-------+\n|summary|  a|  b|      c|\n+-------+---+---+-------+\n|  count|  3|  3|      3|\n|   mean|2.0|3.0|   null|\n| stddev|1.0|1.0|   null|\n|    min|  1|2.0|string1|\n|    max|  3|4.0|string3|\n+-------+---+---+-------+\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.collect()","execution_count":13,"outputs":[{"data":{"text/plain":"[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"},"execution_count":13,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.take(1)","execution_count":14,"outputs":[{"data":{"text/plain":"[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.toPandas()","execution_count":15,"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>string1</td>\n      <td>2000-01-01</td>\n      <td>2000-01-01 12:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.0</td>\n      <td>string2</td>\n      <td>2000-02-01</td>\n      <td>2000-01-02 12:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.0</td>\n      <td>string3</td>\n      <td>2000-03-01</td>\n      <td>2000-01-03 12:00:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   a    b        c           d                   e\n0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"},"execution_count":15,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.a","execution_count":16,"outputs":[{"data":{"text/plain":"Column<b'a'>"},"execution_count":16,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"from pyspark.sql import Column\nfrom pyspark.sql.functions import upper\n\ntype(df.c) == type(upper(df.c)) == type(df.c.isNull())","execution_count":17,"outputs":[{"data":{"text/plain":"True"},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.select(df.c).show()","execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"+-------+\n|      c|\n+-------+\n|string1|\n|string2|\n|string3|\n+-------+\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.withColumn('upper_c', upper(df.c)).show()","execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"+---+---+-------+----------+-------------------+-------+\n|  a|  b|      c|         d|                  e|upper_c|\n+---+---+-------+----------+-------------------+-------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n+---+---+-------+----------+-------------------+-------+\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.filter(df.a == 1).show()","execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\n\n"}]},{"metadata":{},"cell_type":"markdown","source":"## Grouping Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = spark.createDataFrame([\n    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\ndf.show()","execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|  red|banana|  1| 10|\n| blue|banana|  2| 20|\n|  red|carrot|  3| 30|\n| blue| grape|  4| 40|\n|  red|carrot|  5| 50|\n|black|carrot|  6| 60|\n|  red|banana|  7| 70|\n|  red| grape|  8| 80|\n+-----+------+---+---+\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('color').avg().show()","execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"+-----+-------+-------+\n|color|avg(v1)|avg(v2)|\n+-----+-------+-------+\n|  red|    4.8|   48.0|\n|black|    6.0|   60.0|\n| blue|    3.0|   30.0|\n+-----+-------+-------+\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plus_mean(pandas_df):\n    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n\ndf.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()","execution_count":25,"outputs":[{"name":"stdout","output_type":"stream","text":"+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|  red|banana| -3| 10|\n|  red|carrot| -1| 30|\n|  red|carrot|  0| 50|\n|  red|banana|  2| 70|\n|  red| grape|  3| 80|\n|black|carrot|  0| 60|\n| blue|banana| -1| 20|\n| blue| grape|  1| 40|\n+-----+------+---+---+\n\n"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1 = spark.createDataFrame(\n    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    ('time', 'id', 'v1'))\n\ndf2 = spark.createDataFrame(\n    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n    ('time', 'id', 'v2'))\n\ndef merge_ordered(l, r):\n    return pd.merge_ordered(l, r)\n\ndf1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n    merge_ordered, schema='time int, id int, v1 double, v2 string').show()","execution_count":26,"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---+---+---+\n|    time| id| v1| v2|\n+--------+---+---+---+\n|20000101|  1|1.0|  x|\n|20000102|  1|3.0|  x|\n|20000101|  2|2.0|  y|\n|20000102|  2|4.0|  y|\n+--------+---+---+---+\n\n"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"name":"quickstart","notebookId":1927513300154480},"nbformat":4,"nbformat_minor":1}